{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2323e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/burak/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchvision.models.googlenet import InceptionAux\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.hub._validate_not_a_forked_repo=lambda a,b,c: True\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4546c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a doggo image\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09ce56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(filename)\n",
    "\n",
    "num_classes = 21\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "model.cpu()\n",
    "input_batch.cpu()\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "# set model parameters fixed\n",
    "set_parameter_requires_grad(model, True)\n",
    "#model.aux1 = InceptionAux(512, num_classes)\n",
    "#model.aux2 = InceptionAux(528, num_classes)\n",
    "model.fc = nn.Linear(1024, num_classes)\n",
    "model.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "    \n",
    "    # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "\n",
    "\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02652d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(\n",
    "    '/home/burak/Documents/semester_10/remote_sensing/2022-Group06/Presentations/01-Dataset/UCMerced_LandUse/train',\n",
    "    transform=preprocess\n",
    "\n",
    ")\n",
    "valid_dataset = ImageFolder(\n",
    "    '/home/burak/Documents/semester_10/remote_sensing/2022-Group06/Presentations/01-Dataset/UCMerced_LandUse/valid',\n",
    "    transform=preprocess\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# training data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "# validation data loaders\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c9d5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(model, trainloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    print('Training')\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        counter += 1\n",
    "        image, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(image)\n",
    "        # calculate the loss\n",
    "        #loss1 = criterion(outputs[0], labels)\n",
    "        #loss2 = criterion(outputs[1], labels)\n",
    "        #loss3 = criterion(outputs[2], labels)\n",
    "        #loss = loss1 + loss2 + loss3\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_running_loss += loss.item()\n",
    "        # calculate the accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        x = (preds == labels).sum().item()\n",
    "        print(x)\n",
    "        train_running_correct += (preds == labels).sum().item()\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update the optimizer parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    # loss and accuracy for the complete epoch\n",
    "    epoch_loss = train_running_loss / counter\n",
    "    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4206ca2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 2\n",
      "Training\n",
      "90\n",
      "73\n",
      "56\n",
      "83\n",
      "90\n",
      "82\n",
      "79\n",
      "93\n",
      "105\n",
      "101\n",
      "95\n",
      "95\n",
      "97\n",
      "12\n",
      "Training loss: 1.054, training acc: 68.512\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 2 of 2\n",
      "Training\n",
      "111\n",
      "107\n",
      "114\n",
      "110\n",
      "105\n",
      "113\n",
      "106\n",
      "113\n",
      "122\n",
      "118\n",
      "119\n",
      "118\n",
      "112\n",
      "14\n",
      "Training loss: 0.392, training acc: 88.214\n",
      "--------------------------------------------------\n",
      "TRAINING COMPLETE\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# lists to keep track of losses and accuracies\n",
    "train_loss, valid_loss = [], []\n",
    "train_acc, valid_acc = [], []\n",
    "# start the training\n",
    "for epoch in range(epochs):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, train_epoch_acc = train(model, train_loader, \n",
    "                                              optimizer, loss)\n",
    "    #valid_epoch_loss, valid_epoch_acc = validate(model, valid_loader,  \n",
    "    #                                             criterion)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    #valid_loss.append(valid_epoch_loss)\n",
    "    train_acc.append(train_epoch_acc)\n",
    "    #valid_acc.append(valid_epoch_acc)\n",
    "    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "    #print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n",
    "    print('-'*50)\n",
    "    #time.sleep(5)\n",
    "    \n",
    "# save the trained model weights\n",
    "#save_model(epochs, model, optimizer, criterion)\n",
    "# save the loss and accuracy plots\n",
    "#save_plots(train_acc, valid_acc, train_loss, valid_loss)\n",
    "print('TRAINING COMPLETE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d85fd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy 0.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_accuracy = []\n",
    "confusion = np.zeros((21, 21), dtype=int)\n",
    "for images, labels in valid_loader:\n",
    "    output = model(images)\n",
    "    output = torch.argmax(output, 1)\n",
    "    for i in range(len(output)):\n",
    "        confusion[labels[i], output[i]] += 1\n",
    "    test_accuracy.append(output == labels)\n",
    "test_accuracy = torch.cat(test_accuracy).double().mean()\n",
    "print(f\"Test set accuracy {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7178dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
